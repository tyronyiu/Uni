---
templateKey: 'blog-post'
path: /Can_we_teach_morality_to_machines
title: 'Can we teach morality to machines? Three perspectives on ethics for artificial intelligence'
date: 2018-02-01T13:35:10.000Z
description: >- First Post in the discussion page of Contemporary ethics.

---


**Extracts from article for context**

> More often than not, these dystopian prophecies have been met with calls for a more ethical implementation of AI systems; that somehow engineers should imbue autonomous systems with a [sense of ethics](https://standards.ieee.org/develop/indconn/ec/autonomous_systems.html). According to some [AI experts](https://spectrum.ieee.org/podcast/robotics/artificial-intelligence/nick-bostrom-says-we-should-trust-our-future-robot-overlords), we can teach our future robot overlords to tell right from wrong, akin to a “[Good Samaritan AI](https://en.wikipedia.org/wiki/Parable_of_the_Good_Samaritan)” that will always act justly on its own and help humans in distress.

> In [moral dilemmas](https://plato.stanford.edu/entries/moral-dilemmas/), humans tend to rely on gut feeling instead of elaborate cost-benefit calculations. Machines, on the other hand, need explicit and objective metrics that can be clearly measured and optimized.

**A sample scenario:**

> Imagine, in the near future, a bank using a machine learning algorithm to recommend
> mortgage applications for approval. A rejected applicant brings a lawsuit against the
> bank, alleging that the algorithm is discriminating racially against mortgage applicants.
> The bank replies that this is impossible, since the algorithm is deliberately blinded to the
> race of the applicants. Indeed, that was part of the bank’s rationale for implementing
> the system. Even so, statistics show that the bank’s approval rate for black applicants has
> been steadily dropping. Submitting ten apparently equally qualified genuine applicants
> (as determined by a separate panel of human judges) shows that the algorithm accepts
> white applicants and rejects black applicants. What could possibly be happening? —https://intelligence.org/files/EthicsofAI.pdf (Ethics in Machine Learning and Other Domain-Specific AI
> Algorithms)

### Analysis of Article

Dr Vyacheslav Polonski gives three perspectives that could be utilised for implementation into artificial neural networks as possible routes enhancing AI´s ethical awareness. First of all, the article is discussing a solely theoretical scenario based on assumptions of how the future will progress. To this day no real Artificial intelligence has been achieved, but rather an. Despite this fact, normally it will be referred to as "AI". Dr Polonski, as many researchers in his field consider every day situations, in which, mostly due to their uniqueness in terms of variables present, ethical questions become more insightful.

The first perspective **Explicitly defining ethical behaviour** would improve AI´s ethical awareness by adding human input in form of rules to follow in certain situations. As seen the first perspective wants to rather give human made rules than fixing the problem. It seems like a detour from finding an actual solution. Me mentioning an actual solution should not be seen as if there would be one, but rather as an idea, that there would not be enough data to provide for a neural network to make actual assumptions. *Henceforth, all assumptions could be imagined as rounding up or down numbers to different decimal points, the fewer data available, the less precise the results. Now instead of precision, it can be thought of like a four-dimensional object.* Furthermore, by introducing this way of defining any type information as a four-dimensional object (having all dimensions´ attributes), the complexity of the structure of objects that would accumulate in a everyday situation like a mortgage loan decision, would become infinite. The implementation of human guidelines to follow, does not hold the same results as we might believe. The belief, that creating a guideline would actually be understandable in the way we would like it to be, is ridiculous. It is, considering the fact that neural networks are not transparent at all. Due to their process of operating, the progress and process is incomprehensible at real time for a human. This fact is important, as it exposes the fact, that monitoring a neural network would not work in real time at a scale of detail actually understandable for humans. Therefore, we could not monitor what's happening, as well as understanding what is wrong with the neural network, if it is misbehaving and why it is.

The second perspective **crowdsourcing human morality** describes how a consensus (in the article the type of how consensus should be achieved is not stated) on binary options that portray ethical dilemmas would improve ethical implementation into neural networks. The second perspective is again only an input from humans into how a machine should behave. We are not tackling the questions of how to emphasise ethical awareness or trying to "translate" how we perceive ethics, emotionally primarily, due to the amygdala´s priority of transmission of instructions. The fact that the primary ethical decisions are made based upon inherited knowledge, that did or did not effectively adapt to the present day, shows that trying to copy what humans do and try to expect the neural network to do the same. A framework for unbiased mathematical formula should be more appropriate, considering that there are still infinite possible approaches to the same problem, where there are also infinite possible correct answers, but understanding of the current situation is greatly enhanced. Porting emotions and primal instincts to mathematical formulae is a problem to the present day, explaining why no one has done it yet.

The third perspective **Making AI more transparent** was briefly touched on earlier, but essentially not easy to achieve, as AI´s process is too fast for humans to understand in real time, or even to reliably understand past executed code. This fact makes it perfectly clear, that imposing human guidelines is less optimal, than letting the neural network come up with it on its own and simply saying that whatever the network did in a certain situation was appreciated by humans. The execution of human written code is significantly slower than whats made by a neural network, hence trying to expect a neural network to follow a rule that is slower to execute, less likely to succeed, based on how success was defined by human input, would be unsatisfactory.

The biggest problem with ethics and AI is to even make AI aware that there is something such as ethics and morality. These behaviours imprinted themselves over time and besides being incomprehensible to a neural network because of that fact, they are also intangible, unquantifiable and hence not translatable for a neural network. Dr. Polonski and his team created three perspectives, though all of them are trying to raise a neural network like a child instead of a neural network. Their approaches are interesting in the sense that they are the most pursued ones in the field of AI and ethics and even financially backed (MIT moralmachine.mit.edu or my goodness.mit.edu) but are frequently displayed as less likely to actually be solving any questions. E.G: https://openai.com/ Open AI is a panel of researchers including Elon Musk and Peter thief, conducting research on enacting the path for safe artificial general intelligence. Their approach is more theoretical and more mathematical, analysing the processes of neural networks. Neural networks are still extremely narrow in their capabilities and open ai is trying to ensure that human-level AI will not ensure our species´extinction.

References:

https://medium.com/@drpolonski/can-we-teach-morality-to-machines-three-perspectives-on-ethics-for-artificial-intelligence-64fe479e25d3 — main article

http://moralmachine.mit.edu/ —reference for example

https://mygoodness.mit.edu/ — reference for example

https://blog.openai.com/introducing-openai/ —reference for example
